# Project: Build Your Deep Neural Network Step by Step  

**Context**  
Dive into the world of deep neural networks! In this project, you will design and implement a deep neural network with as many layers as you want. After experimenting with a single-hidden-layer network in a previous phase, you’ll now explore multi-layer architectures and non-linear activation functions like ReLU to enhance your model’s performance.  

**Project Goals**  
By completing this project, you will:  
- Integrate non-linear units (e.g., ReLU) to optimize your model’s learning capabilities.  
- Architect a deep neural network with multiple hidden layers.  
- Build a modular neural network class that is easy to use and customize.  
- Lay the groundwork for a follow-up image classification project using your architecture.  

**Structure & Notation**  
To ensure clarity in code and equations, we use the following conventions:  
- Superscript $[l]$: Denotes a parameter associated with layer $l$ (e.g., $a^{[L]}$ is the activation of the $L$-th layer).  
- Superscript $(i)$: Refers to the $i$-th training example (e.g., $x^{(i)}$ is the input of the $i$-th example).  
- Subscript $i$: Represents the $i$-th entry of a vector (e.g., $a^{[l]}_i$ is the $i$-th entry of layer $l$’s activations).  

**Project Steps**  
1. **Core Function Implementation**: Code essential building blocks for deep neural networks (initialization, forward/backward propagation, cost computation).  
2. **Layer Assembly**: Combine these functions to create a flexible architecture adaptable to diverse problems.  
3. **Testing & Validation**: Experiment with synthetic data to verify your network’s functionality before real-world applications.  

**Why This Project Matters**  
Deep neural networks power modern AI (image recognition, NLP, etc.). By building one from scratch, you’ll gain a deep understanding of their inner workings and learn to adapt them to various scenarios.  

**Next Step**  
After completing this project, you’ll reuse your architecture to train an image classification model in Part 2!  



